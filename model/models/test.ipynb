{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the get_mcqa_output function (the parser) from the output_parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\mnlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Tachi67/mnlp_dpo_model_bloom\")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Tachi67/mnlp_dpo_model_bloom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Tachi67/mnlp_dpo_data_7k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_parser import get_mcqa_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Question: The Time-Memory Tradeoff Attack ...\\n\\nOptions:\\nA. is useful for finding a preimage within complexity $O\\\\big(\\\\big({\\\\frac{2}{3}}\\\\big)^N\\\\big).$\\nB. is useful for finding a preimage within complexity $O(N^{\\\\frac{2}{3}}).$\\nC. is a dedicated method which works only on SHA1.\\nD. can be combined with birthday paradox to find the order of the group in RSA efficiently.', 'chosen': 'The Time-Memory Tradeoff Attack is useful for finding a preimage within complexity $O(N^{\\\\frac{2}{3}})$. This attack optimizes the tradeoff between computation time and memory usage to reduce the complexity of finding a preimage in cryptographic hash functions. By strategically utilizing precomputed tables, the attack can achieve a faster time complexity than exhaustive search.', 'rejected': 'The Time-Memory Tradeoff Attack is useful for finding a preimage within complexity $O(N^{\\\\frac{2}{3}})$. By storing intermediate results in memory, this attack method enables a more efficient search for preimages in hash functions. It does not solely apply to SHA1 but can be employed across various cryptographic algorithms where such tradeoffs between time and memory can be leveraged.'}\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(dataset['test']))\n",
    "input_sample = dataset['test'][index]\n",
    "print(input_sample)\n",
    "print(get_mcqa_output(input_sample, model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

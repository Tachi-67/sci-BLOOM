{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\mnlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Tachi67/mnlp_dpo_model_bloom\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Tachi67/mnlp_dpo_model_bloom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 2048)\n",
       "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 2048)\n",
       "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 477/477 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 6.41M/6.41M [00:05<00:00, 1.07MB/s]\n",
      "Downloading data: 100%|██████████| 631k/631k [00:01<00:00, 547kB/s]\n",
      "Generating train split: 100%|██████████| 7000/7000 [00:00<00:00, 233333.56 examples/s]\n",
      "Generating test split: 100%|██████████| 657/657 [00:00<00:00, 164252.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Tachi67/mnlp_dpo_data_7k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_input(input_sample:dict):\n",
    "    prompt = input_sample['prompt']\n",
    "    question_body = prompt.split('Options:')[0].strip()\n",
    "    options = prompt.split('Options:')[1].strip()\n",
    "    \n",
    "    \n",
    "    instruction = f\"\"\"\n",
    "Answer the following question:\n",
    "{question_body}\n",
    "Choose from the following options:\n",
    "Options:\\n{options}\n",
    "    \n",
    "Please provide only one answer with the letter of the option.\n",
    "The chosen option is:\n",
    "    \"\"\"\n",
    "\n",
    "    return instruction, question_body, options\n",
    "\n",
    "def generate_answer(instruction, model, tokenizer, max_new_tokens=10):\n",
    "    input = tokenizer.encode(instruction, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input, max_new_tokens=max_new_tokens)\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mcqa_output(s:str, instruction:str):\n",
    "    s = s[len(instruction):]\n",
    "    def find_alphabetical_answer(s):\n",
    "        # 初始化最佳匹配结果为None\n",
    "        best_index = len(s)  # 初始值设为字符串长度，比任何有效索引都大\n",
    "        best_char = None\n",
    "        characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "        # 遍历每个字符\n",
    "        for char in characters:\n",
    "            index = s.find(char)\n",
    "            if index != -1 and index < best_index:  # 检查是否找到并且是否是更早的索引\n",
    "                best_index = index\n",
    "                best_char = char\n",
    "\n",
    "        # 如果没有找到任何字符，返回-1\n",
    "        if best_char is None:\n",
    "            raise ValueError(\"No alphabetical character found in the string\")\n",
    "\n",
    "        # 返回找到的最早字符的索引\n",
    "        return best_index\n",
    "    return s[find_alphabetical_answer(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "考虑如下情况：\n",
    "\n",
    "model的output是 \n",
    "(a)，或者直接返回了content而不是label（ABCD..）\n",
    "Option: A\n",
    "The chosen option is:\n",
    "     (a) True\n",
    "     (b) False\n",
    "没有回复\n",
    "\n",
    "\n",
    "在原来的算法里，不会捕捉到A。\n",
    "\n",
    "考虑：parse这样的情形；ask again；match content；从option中获取option的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following question:\n",
      "Question: In JOS, after finishing the execution of a user-level page fault handler, how is the program control flow transferred back to the program? (You may get insights from the code snippet of _pagefault_upcall.)?\n",
      "Choose from the following options:\n",
      "Options:\n",
      "A. The control flow will be transferred to kernel first, then to Env that caused the page fault.\n",
      "B. The control flow will be transferred to Env that caused the page fault directly.\n",
      "    \n",
      "Please provide only one answer with the letter of the option.\n",
      "The chosen option is:\n",
      "    \n",
      "===================\n",
      "\n",
      "Answer the following question:\n",
      "Question: In JOS, after finishing the execution of a user-level page fault handler, how is the program control flow transferred back to the program? (You may get insights from the code snippet of _pagefault_upcall.)?\n",
      "Choose from the following options:\n",
      "Options:\n",
      "A. The control flow will be transferred to kernel first, then to Env that caused the page fault.\n",
      "B. The control flow will be transferred to Env that caused the page fault directly.\n",
      "    \n",
      "Please provide only one answer with the letter of the option.\n",
      "The chosen option is:\n",
      "    \n",
      "- _pagefault_upcall: Control flow\n",
      "MCQA ANSWER -----------------> C\n",
      "-------------------\n",
      "Answer the following question:\n",
      "Question: In JOS, after finishing the execution of a user-level page fault handler, how is the program control flow transferred back to the program? (You may get insights from the code snippet of _pagefault_upcall.)?\n",
      "Choose from the following options:\n",
      "Options:\n",
      "A. The control flow will be transferred to kernel first, then to Env that caused the page fault.\n",
      "B. The control flow will be transferred to Env that caused the page fault directly.\n",
      "    \n",
      "Please provide only one answer with the letter of the option.\n",
      "The chosen option is:\n",
      "    \n",
      "C. The control flow will be transferred to\n",
      "MCQA ANSWER -----------------> C\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(dataset['test']))\n",
    "instruction, question_body, options = prep_input(dataset['test'][index])\n",
    "print(instruction)\n",
    "print(\"===================\")\n",
    "\n",
    "output_str_new_model = generate_answer(instruction, model, tokenizer)\n",
    "print(output_str_new_model)\n",
    "print(\"MCQA ANSWER ----------------->\", parse_mcqa_output(output_str_new_model, instruction))\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "output_str_original_model = generate_answer(instruction, original_model, tokenizer)\n",
    "print(output_str_original_model[len(input):])\n",
    "print(\"MCQA ANSWER ----------------->\", parse_mcqa_output(output_str_original_model, instruction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
